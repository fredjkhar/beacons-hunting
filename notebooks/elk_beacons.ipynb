{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beaconing activity detection model\n",
    "\n",
    "The objective of the this notebook is to analyze events to search for potentional beaconing using a statistical approach.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we shall import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dateutil.parser import parse\n",
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we import the .csv file that contains sysmon events collected over a period of approximately two days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = 'elastic'\n",
    "password = 'changeme'\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch([{'host': '34.67.212.1', 'port': 9200, 'scheme': 'http'}])\n",
    "\n",
    "query = {\n",
    "  \"size\": 10000,\n",
    "  \"query\": {\n",
    "    \"range\": {\n",
    "      \"@timestamp\": {\n",
    "        \"gte\": \"now-24h/h\",\n",
    "        \"lt\": \"now/h\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "# Execute the query\n",
    "result = es.search(index='.ds-winlogbeat-*', body=query)\n",
    "\n",
    "# Extract data from the result\n",
    "hits = result['hits']['hits']\n",
    "df = pd.DataFrame([hit['_source'] for hit in hits])\n",
    "\n",
    "print(df.shape[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're currently interested in three columns: the IP addresses of the destination and source, and the time at which each interaction between each unique IP pair took place.\n",
    "\n",
    "We will also save the most recent SystemTime for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the dictionaries into separate columns\n",
    "df_destination = df['destination'].apply(pd.Series)\n",
    "df_source = df['source'].apply(pd.Series)\n",
    "df_process = df['process'].apply(pd.Series)\n",
    "\n",
    "# Rename the columns to avoid name conflicts\n",
    "df_destination = df_destination.rename(columns=lambda x: 'destination.' + str(x))\n",
    "df_source = df_source.rename(columns=lambda x: 'source.' + str(x))\n",
    "df_process = df_process.rename(columns=lambda x: 'process.' + str(x))\n",
    "\n",
    "# Concatenate the expanded columns back to the original DataFrame\n",
    "df = pd.concat([df.drop(['destination', 'source'], axis=1), df_destination, df_source, df_process], axis=1)\n",
    "\n",
    "# Now, filter the DataFrame to include only the target columns\n",
    "target_columns = [\"destination.ip\", \"source.ip\", \"@timestamp\", \"process.name\", \"process.executable\"]\n",
    "df = df[target_columns]\n",
    "\n",
    "# Drop rows with missing (NaN) values\n",
    "df = df.dropna()\n",
    "\n",
    "print(df.shape[0])\n",
    "\n",
    "df_original = df.copy()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by SourceIp and DestinationIp, and calculate the total number of connections and the list of connection times for each unique IP pair\n",
    "df = (\n",
    "    df.groupby([\"source.ip\", \"destination.ip\"])\n",
    "    .agg(\n",
    "        TotalConnections=pd.NamedAgg(column=\"@timestamp\", aggfunc=\"count\"),\n",
    "        ConnectionTimes=pd.NamedAgg(column=\"@timestamp\", aggfunc=list),\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Number of unique host pairs:\", df.shape[0])\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created a dataframe with unique IP pairs as well as the total number of connections for each pair and the datetime of each connection, we move to calculate three scores:\n",
    "* Skew score\n",
    "* MAD score\n",
    "* Count score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with the Skew(ness) score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Skewness score\n",
    "\n",
    "For each IP pair, we need to: \n",
    "* Calculate time intervals between all consecutive connections \n",
    "* Calculate the skewness of these time intervals: A skewness score close to zero could indicate regular, periodic activity, while a high skewness score might suggest more varied or irregular intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bowleys_skewness(connection_times):\n",
    "    # Convert connection times to numpy array\n",
    "    connection_times = np.array(connection_times)\n",
    "\n",
    "    # Sort the connection times and compute intervals (differences)\n",
    "    connection_times.sort()\n",
    "    diffs = np.diff(connection_times).astype(\"timedelta64[s]\").astype(int)\n",
    "\n",
    "    # Data cleansing: Filter out intervals less than 1 second and check if enough data remains\n",
    "    diffs = diffs[diffs > 1]\n",
    "    if len(diffs) < 6:\n",
    "        return None  # Not enough data to compute a score\n",
    "\n",
    "    # Calculate the quartiles\n",
    "    Q1, Q2, Q3 = np.percentile(diffs, [25, 50, 75])\n",
    "\n",
    "    # Compute Bowley's skewness\n",
    "    bowleys_numerator = Q1 + Q3 - 2 * Q2\n",
    "    bowleys_denominator = Q3 - Q1\n",
    "    if bowleys_denominator == 0 or Q2 == Q1 or Q2 == Q3:\n",
    "        return 0  # Handle division by zero and identical quartile cases\n",
    "\n",
    "    bowleys_skewness = bowleys_numerator / bowleys_denominator\n",
    "\n",
    "    # Skew score is the complement of the absolute value of Bowley's skewness to 1\n",
    "    skew_score = 1.0 - abs(bowleys_skewness)\n",
    "    return skew_score\n",
    "\n",
    "# Convert and sort timestamp strings to datetime objects in the \"ConnectionTimes\" column.\n",
    "df[\"ConnectionTimes\"] = df[\"ConnectionTimes\"].apply(\n",
    "    lambda x: sorted([parse(t) for t in x])\n",
    ")\n",
    "\n",
    "# Now apply the `compute_bowleys_skewness` function to each list of connection times\n",
    "df[\"Skew score\"] = df[\"ConnectionTimes\"].apply(compute_bowleys_skewness)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MAD Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mad_score(connection_times):\n",
    "    # Convert connection times to numpy array\n",
    "    connection_times = np.array(connection_times)\n",
    "\n",
    "    # Sort the connection times and compute intervals (differences)\n",
    "    connection_times.sort()\n",
    "    diffs = np.diff(connection_times).astype(\"timedelta64[s]\").astype(int)\n",
    "\n",
    "    # Data cleansing: Filter out intervals less than 1 second and check if enough data remains\n",
    "    diffs = diffs[diffs > 1]\n",
    "    if len(diffs) < 6:\n",
    "        return None  # Not enough data to compute a score\n",
    "\n",
    "    # Calculate the median of the differences\n",
    "    tsMid = np.percentile(diffs, 50)\n",
    "\n",
    "    # Calculate the Median Absolute Deviation (MAD) about the median\n",
    "    tsMadm = np.median(np.abs(diffs - tsMid))\n",
    "\n",
    "    # Normalize the MAD score\n",
    "    tsMadmScore = 1.0 - float(tsMadm) / 30.0\n",
    "\n",
    "    # Ensure the MAD score is non-negative\n",
    "    tsMadmScore = max(tsMadmScore, 0)\n",
    "\n",
    "    return tsMadmScore\n",
    "\n",
    "# Apply the compute_mad_score function to each list of connection times\n",
    "df[\"MAD score\"] = df[\"ConnectionTimes\"].apply(compute_mad_score)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Count score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_connection_count_score(connection_times):\n",
    "    if len(connection_times) < 6:\n",
    "        return None  # Not enough data to compute a meaningful score\n",
    "\n",
    "    connection_times.sort()\n",
    "    # Calculate the total time span of observed connections in seconds\n",
    "    time_span_seconds = (connection_times[-1] - connection_times[0]).total_seconds()\n",
    "    # Calculate the median interval in seconds between connections\n",
    "    diffs = np.diff(connection_times).astype(\"timedelta64[s]\").astype(int)\n",
    "    tsMid = np.percentile(diffs, 50)\n",
    "\n",
    "    # Avoid division by zero in case tsMid is 0\n",
    "    if tsMid == 0:\n",
    "        return 0\n",
    "\n",
    "    # Calculate the expected number of connections based on the median interval and the actual time span\n",
    "    tsTimespanDiv = float(time_span_seconds) / tsMid\n",
    "    # Calculate the connection count score based on the actual and expected number of connections\n",
    "    tsConnCountScore = float(len(connection_times)) / tsTimespanDiv\n",
    "\n",
    "    # Cap the score at 1.0\n",
    "    tsConnCountScore = min(tsConnCountScore, 1.0)\n",
    "\n",
    "    return tsConnCountScore\n",
    "\n",
    "df[\"Count score\"] = df[\"ConnectionTimes\"].apply(compute_connection_count_score)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Compute the final score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_combined_score(row):\n",
    "    return round(((row[\"Skew score\"] + row[\"MAD score\"] + row[\"Count score\"]) / 3), 4)\n",
    "\n",
    "df = df.drop(columns=[\"ConnectionTimes\"])\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Compute the combined score for each unique host pair\n",
    "df[\"Score\"] = df.apply(compute_combined_score, axis=1)\n",
    "\n",
    "# Sort the dataframe by the score in descending order\n",
    "df.sort_values(\"Score\", ascending=False, inplace=True)\n",
    "\n",
    "print('number of rows', df.shape[0])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_details = df.merge(\n",
    "    df_original[\n",
    "        [\"destination.ip\", \"source.ip\", \"@timestamp\", \"process.name\", \"process.executable\"]\n",
    "    ],\n",
    "    on=[\"source.ip\", \"destination.ip\"],\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "print(df_with_details.columns)\n",
    "\n",
    "df.drop_duplicates(\n",
    "    subset=[\"source.ip\", \"destination.ip\"], keep=\"first\", inplace=True\n",
    ")\n",
    "\n",
    "df = (\n",
    "    df_with_details.groupby([\"source.ip\", \"destination.ip\"])\n",
    "    .agg(\n",
    "        {\n",
    "            \"TotalConnections\": \"first\",\n",
    "            \"Score\": \"first\",\n",
    "            \"Skew score\": \"first\",\n",
    "            \"MAD score\": \"first\",\n",
    "            \"Count score\": \"first\",\n",
    "            \"process.name\": lambda x: \" | \".join(\n",
    "                set(x.dropna().astype(str))\n",
    "            ),  # Join unique fileNames after converting to string\n",
    "            \"process.executable\": lambda x: \" | \".join(\n",
    "                set(x.dropna().astype(str))\n",
    "            ),  # Join unique commandLines after converting to string\n",
    "        }\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df.sort_values(\"Score\", ascending=False, inplace=True)\n",
    "\n",
    "df.to_csv(\"./Results/final_results.csv\")\n",
    "\n",
    "print(df.shape[0])\n",
    "\n",
    "df.head(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
